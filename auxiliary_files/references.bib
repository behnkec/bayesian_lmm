@article{baayen_etal08,
  title = {Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items},
  author = {Baayen, R. H. and Davidson, D. J. and Bates, D. M.},
  date = {2008-11-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  series = {Special {{Issue}}: {{Emerging Data Analysis}}},
  volume = {59},
  number = {4},
  pages = {390--412},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.12.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X07001398},
  urldate = {2024-08-07},
  abstract = {This paper provides an introduction to mixed-effects models for the analysis of repeated measurement data with subjects and items as crossed random effects. A worked-out example of how to use recent software for mixed-effects modeling is provided. Simulation studies illustrate the advantages offered by mixed-effects analyses compared to traditional analyses based on quasi-F tests, by-subjects analyses, combined by-subjects and by-items analyses, and random regression. Applications and possibilities across a range of domains of inquiry are discussed.},
  keywords = {By-item,By-subject,Crossed random effects,Mixed-effects models,Quasi-F},
  file = {/Users/clarabehnke/Zotero/storage/D9XWLXMF/Baayen et al. - 2008 - Mixed-effects modeling with crossed random effects.pdf;/Users/clarabehnke/Zotero/storage/7KZPZ4XY/S0749596X07001398.html}
}

@article{bagiella_etal00,
  title = {Mixed-Effects Models in Psychophysiology},
  author = {Bagiella, Emilia and Sloan, Richard P. and Heitjan, Daniel F.},
  date = {2000},
  journaltitle = {Psychophysiology},
  volume = {37},
  number = {1},
  pages = {13--20},
  issn = {1469-8986},
  doi = {10.1111/1469-8986.3710013},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1469-8986.3710013},
  urldate = {2024-09-23},
  abstract = {The current methodological policy in Psychophysiology stipulates that repeated-measures designs be analyzed using either multivariate analysis of variance (ANOVA) or repeated-measures ANOVA with the Greenhouse–Geisser or Huynh–Feldt correction. Both techniques lead to appropriate type I error probabilities under general assumptions about the variance-covariance matrix of the data. This report introduces mixed-effects models as an alternative procedure for the analysis of repeated-measures data in Psychophysiology. Mixed-effects models have many advantages over the traditional methods: They handle missing data more effectively and are more efficient, parsimonious, and flexible. We described mixed-effects modeling and illustrated its applicability with a simple example.},
  langid = {english},
  keywords = {Mixed effects models,Repeated measures designs,Variance-covariance matrix},
  file = {/Users/clarabehnke/Zotero/storage/BYDSX7IU/Bagiella et al. - 2000 - Mixed-effects models in psychophysiology.pdf;/Users/clarabehnke/Zotero/storage/8TUQEBMV/1469-8986.html}
}

@article{barr_etal13,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  date = {2013-04-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {68},
  number = {3},
  pages = {255--278},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.11.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X12001180},
  urldate = {2024-09-25},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
  keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,Statistics},
  file = {/Users/clarabehnke/Zotero/storage/T3VDWJEK/Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf;/Users/clarabehnke/Zotero/storage/LS3WVQCB/S0749596X12001180.html}
}

@online{bates_etal18,
  title = {Parsimonious {{Mixed Models}}},
  author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
  date = {2018-05-26},
  eprint = {1506.04967},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1506.04967},
  url = {http://arxiv.org/abs/1506.04967},
  urldate = {2024-08-07},
  abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.},
  pubstate = {prepublished},
  keywords = {Statistics - Methodology},
  file = {/Users/clarabehnke/Zotero/storage/PMTKWLPB/Bates et al. - 2018 - Parsimonious Mixed Models.pdf;/Users/clarabehnke/Zotero/storage/BSEAY7QH/1506.html}
}

@online{betancourt18,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  date = {2018-07-15},
  eprint = {1701.02434},
  eprinttype = {arXiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2024-05-05},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Statistics - Methodology},
  file = {/Users/clarabehnke/Zotero/storage/YHCI2JP3/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf}
}

@online{brandmaier_peikert24,
  title = {Automated {{Reproducibility Testing}} in {{R Markdown}}},
  author = {Brandmaier, Andreas Markus and Peikert, Aaron},
  date = {2024-06-27},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/3zjvf},
  url = {https://osf.io/3zjvf},
  urldate = {2024-07-02},
  abstract = {Computational results are considered \_reproducible\_ if the same computation on the same data yields the same results if performed on a different computer or on the same computer later in time. Reproducibility is a prerequisite for replicable, robust and transparent research in digital environments. Various approaches have been suggested to increase chances of reproducibility. Many of them rely on R Markdown as a language to dynamically generate reproducible research assets (e.g., reports, posters, or presentations).  However, a simple way to detect non-reproducibility, that is, unwanted changes in these assets over time is still missing. We introduce the R package `reproducibleRchunks`, which provides a new type of code chunk in R Markdown documents, which automatically stores meta data about original computational results and verifies later reproduction attempts. With a minimal change to users' workflows, we hope that this approach increases transparency and trustworthiness of digital research assets.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {computation,Markdown,Open Science,R,reproducibility,statistical analysis}
}

@article{brauer_curtin18,
  title = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data: {{A}} Unified Framework to Analyze Categorical and Continuous Independent Variables That Vary within-Subjects and/or within-Items},
  shorttitle = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data},
  author = {Brauer, Markus and Curtin, John J.},
  date = {2018-09},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {389--411},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/met0000159},
  url = {https://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2017-52405-001&site=ehost-live},
  urldate = {2024-08-07},
  abstract = {In this article we address a number of important issues that arise in the analysis of nonindependent data. Such data are common in studies in which predictors vary within 'units' (e.g., within-subjects, within-classrooms). Most researchers analyze categorical within-unit predictors with repeated-measures ANOVAs, but continuous within-unit predictors with linear mixed-effects models (LMEMs). We show that both types of predictor variables can be analyzed within the LMEM framework. We discuss designs with multiple sources of nonindependence, for example, studies in which the same subjects rate the same set of items or in which students nested in classrooms provide multiple answers. We provide clear guidelines about the types of random effects that should be included in the analysis of such designs. We also present a number of corrective steps that researchers can take when convergence fails in LMEM models with too many parameters. We end with a brief discussion on the trade-off between power and generalizability in designs with 'within-unit' predictors. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Classrooms,convergence problems,fixed and random effects,Independent Variables,linear mixed-effects models,Maximum Likelihood,Models,Repeated Measures,Statistical Analysis,Statistical Power,The analysis of nonindependent data,within-subjects designs},
  file = {/Users/clarabehnke/Zotero/storage/6X9V7MBT/Brauer und Curtin - 2018 - Linear mixed-effects models and the analysis of no.pdf}
}

@article{brown21,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  date = {2021-01-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  url = {https://doi.org/10.1177/2515245920960351},
  urldate = {2024-08-07},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/7XRPWH8Z/Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling i.pdf}
}

@article{burki_etal18,
  title = {Accounting for Stimulus and Participant Effects in Event-Related Potential Analyses to Increase the Replicability of Studies},
  author = {Bürki, Audrey and Frossard, Jaromil and Renaud, Olivier},
  date = {2018-11-01},
  journaltitle = {Journal of Neuroscience Methods},
  shortjournal = {Journal of Neuroscience Methods},
  volume = {309},
  pages = {218--227},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2018.09.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0165027018302772},
  urldate = {2024-08-28},
  abstract = {Background Event-related potentials (ERPs) are increasingly used in cognitive science. With their high temporal resolution, they offer a unique window into cognitive processes and their time course. In this paper, we focus on ERP experiments whose designs involve selecting participants and stimuli amongst many. Recently, Westfall et al. (2017) highlighted the drastic consequences of not considering stimuli as a random variable in fMRI studies with such designs. Most ERP studies in cognitive psychology suffer from the same drawback. New method We advocate the use of the Quasi-F or Mixed-effects models instead of the classical ANOVA/by-participant F1 statistic to analyze ERP datasets in which the dependent variable is reduced to one measure per trial (e.g., mean amplitude). We combine Quasi-F statistic and cluster mass tests to analyze datasets with multiple measures per trial. Doing so allows us to treat stimulus as a random variable while correcting for multiple comparisons. Results Simulations show that the use of Quasi-F statistics with cluster mass tests allows maintaining the family wise error rates close to the nominal alpha level of 0.05. Comparison with existing methods Simulations reveal that the classical ANOVA/F1 approach has an alarming FWER, demonstrating the superiority of models that treat both participant and stimulus as random variables, like the Quasi-F approach. Conclusions Our simulations question the validity of studies in which stimulus is not treated as a random variable. Failure to change the current standards feeds the replicability crisis.},
  keywords = {Cluster mass,ERP,Mixed-effects model,Quasi-F,Replicability crisis,Stimulus as fixed-effect fallacy},
  file = {/Users/clarabehnke/Zotero/storage/2DSSCQNC/S0165027018302772.html}
}

@article{burkner18,
  title = {Advanced {{Bayesian Multilevel Modeling}} with the {{R Package}} Brms},
  author = {Bürkner, Paul-Christian},
  date = {2018},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume = {10},
  number = {1},
  pages = {395},
  issn = {2073-4859},
  doi = {10.32614/RJ-2018-017},
  url = {https://journal.r-project.org/archive/2018/RJ-2018-017/index.html},
  urldate = {2024-04-18},
  abstract = {The brms package allows R users to easily specify a wide range of Bayesian single-level and multilevel models which are fit with the probabilistic programming language Stan behind the scenes. Several response distributions are supported, of which all parameters (e.g., location, scale, and shape) can be predicted. Non-linear relationships may be specified using non-linear predictor terms or semi-parametric approaches such as splines or Gaussian processes. Multivariate models can be fit as well. To make all of these modeling options possible in a multilevel framework, brms provides an intuitive and powerful formula syntax, which extends the well known formula syntax of lme4. The purpose of the present paper is to introduce this syntax in detail and to demonstrate its usefulness with four examples, each showing relevant aspects of the syntax.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/87IPFU2S/Bürkner - 2018 - Advanced Bayesian Multilevel Modeling with the R P.pdf}
}

@online{burkner24,
  title = {Estimating {{Distributional Models}} with Brms},
  author = {Bürkner, Paul-Christian},
  date = {2024-03-19},
  url = {https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html}
}

@article{carpenter_etal17,
  title = {\emph{Stan} : {{A Probabilistic Programming Language}}},
  shorttitle = {\emph{Stan}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {76},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {http://www.jstatsoft.org/v76/i01/},
  urldate = {2024-08-15},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/TAC5JQLV/Carpenter et al. - 2017 - Stan  A Probabilistic Programming Language.pdf}
}

@article{depaoli_etal17,
  title = {An Introduction to {{Bayesian}} Statistics in Health Psychology},
  author = {Depaoli, Sarah and Rus, Holly M. and Clifton, James P. and family=Schoot, given=Rens, prefix=van de, useprefix=true and Tiemensma, Jitske},
  date = {2017-07-03},
  journaltitle = {Health Psychology Review},
  volume = {11},
  number = {3},
  eprint = {28633558},
  eprinttype = {pmid},
  pages = {248--264},
  publisher = {Routledge},
  issn = {1743-7199},
  doi = {10.1080/17437199.2017.1343676},
  url = {https://doi.org/10.1080/17437199.2017.1343676},
  urldate = {2024-09-24},
  abstract = {The aim of the current article is to provide a brief introduction to Bayesian statistics within the field of health psychology. Bayesian methods are increasing in prevalence in applied fields, and they have been shown in simulation research to improve the estimation accuracy of structural equation models, latent growth curve (and mixture) models, and hierarchical linear models. Likewise, Bayesian methods can be used with small sample sizes since they do not rely on large sample theory. In this article, we discuss several important components of Bayesian statistics as they relate to health-based inquiries. We discuss the incorporation and impact of prior knowledge into the estimation process and the different components of the analysis that should be reported in an article. We present an example implementing Bayesian estimation in the context of blood pressure changes after participants experienced an acute stressor. We conclude with final thoughts on the implementation of Bayesian statistics in health psychology, including suggestions for reviewing Bayesian manuscripts and grant proposals. We have also included an extensive amount of online supplementary material to complement the content presented here, including Bayesian examples using many different software programmes and an extensive sensitivity analysis examining the impact of priors.},
  keywords = {Bayesian statistics,convergence,posterior,prior distributions},
  file = {/Users/clarabehnke/Zotero/storage/RAPT5L8V/Depaoli et al. - 2017 - An introduction to Bayesian statistics in health p.pdf}
}

@article{eimer11a,
  title = {The {{Face-Sensitivity}} of the {{N170 Component}}},
  author = {Eimer, Martin},
  date = {2011-10-18},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front Hum Neurosci},
  volume = {5},
  eprint = {22022313},
  eprinttype = {pmid},
  pages = {119},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2011.00119},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3196313/},
  urldate = {2024-09-25},
  pmcid = {PMC3196313},
  file = {/Users/clarabehnke/Zotero/storage/UT2K3FKR/Eimer - 2011 - The Face-Sensitivity of the N170 Component.pdf}
}

@article{enge_etal23,
  title = {Instant {{Effects}} of {{Semantic Information}} on {{Visual Perception}}},
  author = {Enge, Alexander and Süß, Franziska and Rahman, Rasha Abdel},
  date = {2023-06-28},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {43},
  number = {26},
  eprint = {37286353},
  eprinttype = {pmid},
  pages = {4896--4906},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2038-22.2023},
  url = {https://www.jneurosci.org/content/43/26/4896},
  urldate = {2024-10-25},
  abstract = {Does our perception of an object change once we discover what function it serves? We showed human participants (n = 48, 31 females and 17 males) pictures of unfamiliar objects either together with keywords matching their function, leading to semantically informed perception, or together with nonmatching keywords, resulting in uninformed perception. We measured event-related potentials to investigate at which stages in the visual processing hierarchy these two types of object perception differed from one another. We found that semantically informed compared with uninformed perception was associated with larger amplitudes in the N170 component (150-200 ms), reduced amplitudes in the N400 component (400-700 ms), and a late decrease in alpha/beta band power. When the same objects were presented once more without any information, the N400 and event-related power effects persisted, and we also observed enlarged amplitudes in the P1 component (100-150 ms) in response to objects for which semantically informed perception had taken place. Consistent with previous work, this suggests that obtaining semantic information about previously unfamiliar objects alters aspects of their lower-level visual perception (P1 component), higher-level visual perception (N170 component), and semantic processing (N400 component, event-related power). Our study is the first to show that such effects occur instantly after semantic information has been provided for the first time, without requiring extensive learning. SIGNIFICANCE STATEMENT There has been a long-standing debate about whether or not higher-level cognitive capacities, such as semantic knowledge, can influence lower-level perceptual processing in a top-down fashion. Here we could show, for the first time, that information about the function of previously unfamiliar objects immediately influences cortical processing within less than 200 ms. Of note, this influence does not require training or experience with the objects and related semantic information. Therefore, our study is the first to show effects of cognition on perception while ruling out the possibility that prior knowledge merely acts by preactivating or altering stored visual representations. Instead, this knowledge seems to alter perception online, thus providing a compelling case against the impenetrability of perception by cognition.},
  langid = {english},
  keywords = {event-related potentials,objects,semantic knowledge,visual perception},
  file = {/Users/clarabehnke/Zotero/storage/G75QACUV/Enge et al. - 2023 - Instant Effects of Semantic Information on Visual .pdf}
}

@article{epskamp19,
  title = {Reproducibility and {{Replicability}} in a {{Fast-Paced Methodological World}}},
  author = {Epskamp, Sacha},
  date = {2019-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {145--155},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245919847421},
  url = {https://doi.org/10.1177/2515245919847421},
  urldate = {2024-08-20},
  abstract = {Methodological developments and software implementations are progressing at an increasingly fast pace. The introduction and widespread acceptance of preprint archived reports and open-source software have made state-of-the-art statistical methods readily accessible to researchers. At the same time, researchers are increasingly concerned that their results should be reproducible (i.e., the same analysis should yield the same numeric results at a later time), which is a basic requirement for assessing the results’ replicability (i.e., whether results at a later time support the same conclusions). Although this age of fast-paced methodology greatly facilitates reproducibility and replicability, it also undermines them in ways not often realized by researchers. This article draws researchers’ attention to these threats and proposes guidelines to help minimize their impact. Reproducibility may be influenced by software development and change over time, a problem that is greatly compounded by the rising dependency between software packages. Replicability is affected by rapidly changing standards, researcher degrees of freedom, and possible bugs or errors in code, whether introduced by software developers or empirical researchers implementing an analysis. This article concludes with a list of recommendations to improve the reproducibility and replicability of results.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/4YC6CIBB/Epskamp - 2019 - Reproducibility and Replicability in a Fast-Paced .pdf}
}

@article{field_wright11,
  title = {A {{Primer}} on {{Using Multilevel Models}} in {{Clinical}} and {{Experimental Psychopathology Research}}},
  author = {Field, Andy P. and Wright, Daniel B.},
  date = {2011-05-01},
  journaltitle = {Journal of Experimental Psychopathology},
  volume = {2},
  number = {2},
  pages = {271--293},
  publisher = {SAGE Publications},
  issn = {2043-8087},
  doi = {10.5127/jep.013711},
  url = {https://doi.org/10.5127/jep.013711},
  urldate = {2024-11-08},
  abstract = {A Multilevel model is a statistical tool for analysing data that has a hierarchical data structure (in other words, data are nested within contexts). This paper describes what a multilevel model is, how it is described mathematically, the advantages of using this data analysis technique, and some practical issues to consider. We then move on to describe the application of multilevel models using two scenarios pertinent to researchers interested in clinical trial analysis and experimental psychopathology research. We describe how to use the software R to run these analyses.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/STT6ZRSD/Field und Wright - 2011 - A Primer on Using Multilevel Models in Clinical an.pdf}
}

@article{fromer_etal18,
  title = {Group-Level {{EEG-processing}} Pipeline for Flexible Single Trial-Based Analyses Including Linear Mixed Models},
  author = {Frömer, Romy and Maier, Martin and Abdel Rahman, Rasha},
  date = {2018},
  journaltitle = {Frontiers in Neuroscience},
  volume = {12},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00048},
  urldate = {2023-10-10},
  abstract = {Here we present an application of an EEG processing pipeline customizing EEGLAB and FieldTrip functions, specifically optimized to flexibly analyze EEG data based on single trial information. The key component of our approach is to create a comprehensive 3-D EEG data structure including all trials and all participants maintaining the original order of recording. This allows straightforward access to subsets of the data based on any information available in a behavioral data structure matched with the EEG data (experimental conditions, but also performance indicators, such accuracy or RTs of single trials). In the present study we exploit this structure to compute linear mixed models (LMMs, using lmer in R) including random intercepts and slopes for items. This information can easily be read out from the matched behavioral data, whereas it might not be accessible in traditional ERP approaches without substantial effort. We further provide easily adaptable scripts for performing cluster-based permutation tests (as implemented in FieldTrip), as a more robust alternative to traditional omnibus ANOVAs. Our approach is particularly advantageous for data with parametric within-subject covariates (e.g., performance) and/or multiple complex stimuli (such as words, faces or objects) that vary in features affecting cognitive processes and ERPs (such as word frequency, salience or familiarity), which are sometimes hard to control experimentally or might themselves constitute variables of interest. The present dataset was recorded from 40 participants who performed a visual search task on previously unfamiliar objects, presented either visually intact or blurred. MATLAB as well as R scripts are provided that can be adapted to different datasets.},
  file = {/Users/clarabehnke/Zotero/storage/HPBWQ49N/Frömer et al. - 2018 - Group-Level EEG-Processing Pipeline for Flexible S.pdf}
}

@article{gelman_etal14,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  date = {2014-11-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {24},
  number = {6},
  pages = {997--1016},
  issn = {1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  url = {https://doi.org/10.1007/s11222-013-9416-2},
  urldate = {2024-09-09},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  langid = {english},
  keywords = {AIC,Artificial Intelligence,Bayes,Cross-validation,DIC,Prediction,WAIC},
  file = {/Users/clarabehnke/Zotero/storage/KEGMKEJK/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf}
}

@book{gelman_etal15,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  date = {2015-07-06},
  edition = {3},
  publisher = {{Chapman and Hall/CRC}},
  location = {New York},
  doi = {10.1201/b16018},
  abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian AnalysisNow in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied},
  isbn = {978-0-429-11307-9},
  pagetotal = {675}
}

@article{gelman_etal17,
  title = {The {{Prior Can Often Only Be Understood}} in the {{Context}} of the {{Likelihood}}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  date = {2017-10},
  journaltitle = {Entropy},
  volume = {19},
  number = {10},
  pages = {555},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e19100555},
  url = {https://www.mdpi.com/1099-4300/19/10/555},
  urldate = {2024-08-08},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  issue = {10},
  langid = {english},
  keywords = {Bayesian inference,default priors,prior distribution},
  file = {/Users/clarabehnke/Zotero/storage/T4H4IFRQ/Gelman et al. - 2017 - The Prior Can Often Only Be Understood in the Cont.pdf}
}

@online{gelmana,
  title = {When {{MCMC}} Fails: {{The}} Advice We’re Giving Is Wrong. {{Here}}’s What We You Should Be Doing Instead. ({{Hint}}: It’s All about the Folk Theorem.) | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  author = {Gelman, Andrew},
  url = {https://statmodeling.stat.columbia.edu/2021/06/10/when-mcmc-fails-the-advice-were-giving-is-wrong-heres-what-we-you-should-be-doing-instead-hint-its-all-about-the-folk-theorem/},
  urldate = {2024-10-29},
  file = {/Users/clarabehnke/Zotero/storage/KZECGKQG/when-mcmc-fails-the-advice-were-giving-is-wrong-heres-what-we-you-should-be-doing-instead-hint-.html}
}

@article{gronau_etal17,
  title = {A Tutorial on Bridge Sampling},
  author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
  date = {2017-12-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {80--97},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249617300640},
  urldate = {2024-10-30},
  abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng \& Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model—a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
  keywords = {Bayes factor,Hierarchical model,Marginal likelihood,Normalizing constant,Predictive accuracy,Reinforcement learning},
  file = {/Users/clarabehnke/Zotero/storage/93AFR6UP/Gronau et al. - 2017 - A tutorial on bridge sampling.pdf;/Users/clarabehnke/Zotero/storage/GR7WG822/S0022249617300640.html}
}

@article{gronau_etal20,
  title = {Bridgesampling: {{An R Package}} for {{Estimating Normalizing Constants}}},
  shorttitle = {Bridgesampling},
  author = {Gronau, Quentin F. and Singmann, Henrik and Wagenmakers, Eric-Jan},
  date = {2020-02-27},
  journaltitle = {Journal of Statistical Software},
  volume = {92},
  pages = {1--29},
  issn = {1548-7660},
  doi = {10.18637/jss.v092.i10},
  url = {https://doi.org/10.18637/jss.v092.i10},
  urldate = {2024-10-30},
  abstract = {Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve highdimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng and Wong 1996; Meng and Schilling 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples.},
  langid = {english},
  keywords = {Bayes factor,bridge sampling,marginal likelihood,model selection,Warp-III},
  file = {/Users/clarabehnke/Zotero/storage/8LRWN5XU/Gronau et al. - 2020 - bridgesampling An R Package for Estimating Normal.pdf}
}

@article{gronau_wagenmakers19,
  title = {Limitations of {{Bayesian Leave-One-Out Cross-Validation}} for {{Model Selection}}},
  author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan},
  date = {2019-03-01},
  journaltitle = {Computational Brain \& Behavior},
  shortjournal = {Comput Brain Behav},
  volume = {2},
  number = {1},
  pages = {1--11},
  issn = {2522-087X},
  doi = {10.1007/s42113-018-0011-7},
  url = {https://doi.org/10.1007/s42113-018-0011-7},
  urldate = {2024-09-09},
  abstract = {Cross-validation (CV) is increasingly popular as a generic method to adjudicate between mathematical models of cognition and behavior. In order to measure model generalizability, CV quantifies out-of-sample predictive performance, and the CV preference goes to the model that predicted the out-of-sample data best. The advantages of CV include theoretic simplicity and practical feasibility. Despite its prominence, however, the limitations of CV are often underappreciated. Here, we demonstrate the limitations of a particular form of CV—Bayesian leave-one-out cross-validation or LOO—with three concrete examples. In each example, a data set of infinite size is perfectly in line with the predictions of a simple model (i.e., a general law or invariance). Nevertheless, LOO shows bounded and relatively modest support for the simple model. We conclude that CV is not a panacea for model selection.},
  langid = {english},
  keywords = {Bounded support,Consistency,Evidence,Generalizability,Induction,Principle of parsimony},
  file = {/Users/clarabehnke/Zotero/storage/68UFD3QF/Gronau und Wagenmakers - 2019 - Limitations of Bayesian Leave-One-Out Cross-Valida.pdf}
}

@article{halsey_etal15,
  title = {The Fickle {{P}} Value Generates Irreproducible Results},
  author = {Halsey, Lewis G. and Curran-Everett, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
  date = {2015-03},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {12},
  number = {3},
  pages = {179--185},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3288},
  url = {https://www.nature.com/articles/nmeth.3288},
  urldate = {2024-11-01},
  abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
  langid = {english},
  keywords = {Biological techniques,Education,Medical research,Statistical methods},
  file = {/Users/clarabehnke/Zotero/storage/7L8S8TC5/Halsey et al. - 2015 - The fickle P value generates irreproducible result.pdf}
}

@article{hansen_etal22,
  title = {How to Conduct a Meta-Analysis in Eight Steps: A Practical Guide},
  shorttitle = {How to Conduct a Meta-Analysis in Eight Steps},
  author = {Hansen, Christopher and Steinmetz, Holger and Block, Jörn},
  date = {2022-02-01},
  journaltitle = {Management Review Quarterly},
  shortjournal = {Manag Rev Q},
  volume = {72},
  number = {1},
  pages = {1--19},
  issn = {2198-1639},
  doi = {10.1007/s11301-021-00247-4},
  url = {https://doi.org/10.1007/s11301-021-00247-4},
  urldate = {2024-08-20},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/5B3BSS2D/Hansen et al. - 2022 - How to conduct a meta-analysis in eight steps a p.pdf}
}

@article{heck_etal23,
  title = {A Review of Applications of the {{Bayes}} Factor in Psychological Research},
  author = {Heck, Daniel W. and Boehm, Udo and Böing-Messing, Florian and Bürkner, Paul-Christian and Derks, Koen and Dienes, Zoltan and Fu, Qianrao and Gu, Xin and Karimova, Diana and Kiers, Henk A. L. and Klugkist, Irene and Kuiper, Rebecca M. and Lee, Michael D. and Leenders, Roger and Leplaa, Hidde J. and Linde, Maximilian and Ly, Alexander and Meijerink-Bosman, Marlyne and Moerbeek, Mirjam and Mulder, Joris and Palfi, Bence and Schönbrodt, Felix D. and Tendeiro, Jorge N. and family=Bergh, given=Don, prefix=van den, useprefix=true and Van Lissa, Caspar J. and family=Ravenzwaaij, given=Don, prefix=van, useprefix=true and Vanpaemel, Wolf and Wagenmakers, Eric-Jan and Williams, Donald R. and Zondervan-Zwijnenburg, Mariëlle and Hoijtink, Herbert},
  date = {2023},
  journaltitle = {Psychological Methods},
  volume = {28},
  number = {3},
  pages = {558--579},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000454},
  abstract = {The last 25 years have shown a steady increase in attention for the Bayes factor as a tool for hypothesis evaluation and model selection. The present review highlights the potential of the Bayes factor in psychological research. We discuss six types of applications: Bayesian evaluation of point null, interval, and informative hypotheses, Bayesian evidence synthesis, Bayesian variable selection and model averaging, and Bayesian evaluation of cognitive models. We elaborate what each application entails, give illustrative examples, and provide an overview of key references and software with links to other applications. The article is concluded with a discussion of the opportunities and pitfalls of Bayes factor applications and a sketch of corresponding future research lines. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Bayesian Analysis,Hypothesis Testing,Mental Models,Statistical Probability},
  file = {/Users/clarabehnke/Zotero/storage/2GGWF7QG/Heck et al. - 2023 - A review of applications of the Bayes factor in ps.pdf}
}

@article{itier_taylor04,
  title = {N170 or {{N1}}? {{Spatiotemporal Differences}} between {{Object}} and {{Face Processing Using ERPs}}},
  author = {Itier, Roxane J. and Taylor, Margot J.},
  date = {2004-02-01},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cerebral Cortex},
  volume = {14},
  number = {2},
  pages = {132--142},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhg111},
  url = {https://doi.org/10.1093/cercor/bhg111},
  urldate = {2024-10-23},
  abstract = {The ERP component N170 is face-sensitive, yet its specificity for faces is controversial. We recorded ERPs while subjects viewed upright and inverted faces and seven object categories. Peak, topography and segmentation analyses were performed. N170 was earlier and larger to faces than to all objects. The classic increase in amplitude and latency was found for inverted faces on N170 but also on P1. Segmentation analyses revealed an extra map found only for faces, reflecting an extra cluster of activity compared to objects. While the N1 for objects seems to reflect the return to baseline from the P1, the N170 for faces reflects a supplement activity. The electrophysiological ‘specificity’ of faces could lie in the involvement of extra generators for face processing compared to objects and the N170 for faces seems qualitatively different from the N1 for objects. Object and face processing also differed as early as 120 ms.}
}

@article{joe06,
  title = {Generating Random Correlation Matrices Based on Partial Correlations},
  author = {Joe, Harry},
  date = {2006-11-01},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  volume = {97},
  number = {10},
  pages = {2177--2189},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2005.05.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X05000886},
  urldate = {2024-10-15},
  abstract = {A d-dimensional positive definite correlation matrix R=(ρij) can be parametrized in terms of the correlations ρi,i+1 for i=1,…,d-1, and the partial correlations ρij|i+1,…j-1 for j-i⩾2. These d2 parameters can independently take values in the interval (-1,1). Hence we can generate a random positive definite correlation matrix by choosing independent distributions Fij, 1⩽i},
  keywords = {Beta distribution,Determinant of correlation matrix},
  file = {/Users/clarabehnke/Zotero/storage/NAH6QPCV/S0047259X05000886.html}
}

@article{judd_etal12,
  title = {Treating Stimuli as a Random Factor in Social Psychology: A New and Comprehensive Solution to a Pervasive but Largely Ignored Problem},
  shorttitle = {Treating Stimuli as a Random Factor in Social Psychology},
  author = {Judd, Charles M. and Westfall, Jacob and Kenny, David A.},
  date = {2012-07},
  journaltitle = {Journal of Personality and Social Psychology},
  shortjournal = {J Pers Soc Psychol},
  volume = {103},
  number = {1},
  eprint = {22612667},
  eprinttype = {pmid},
  pages = {54--69},
  issn = {1939-1315},
  doi = {10.1037/a0028347},
  abstract = {Throughout social and cognitive psychology, participants are routinely asked to respond in some way to experimental stimuli that are thought to represent categories of theoretical interest. For instance, in measures of implicit attitudes, participants are primed with pictures of specific African American and White stimulus persons sampled in some way from possible stimuli that might have been used. Yet seldom is the sampling of stimuli taken into account in the analysis of the resulting data, in spite of numerous warnings about the perils of ignoring stimulus variation (Clark, 1973; Kenny, 1985; Wells \& Windschitl, 1999). Part of this failure to attend to stimulus variation is due to the demands imposed by traditional analysis of variance procedures for the analysis of data when both participants and stimuli are treated as random factors. In this article, we present a comprehensive solution using mixed models for the analysis of data with crossed random factors (e.g., participants and stimuli). We show the substantial biases inherent in analyses that ignore one or the other of the random factors, and we illustrate the substantial advantages of the mixed models approach with both hypothetical and actual, well-known data sets in social psychology (Bem, 2011; Blair, Chapleau, \& Judd, 2005; Correll, Park, Judd, \& Wittenbrink, 2002).},
  langid = {english},
  keywords = {Analysis of Variance,Cues,Humans,Psychology Social,Research Design}
}

@article{kappenman_etal21,
  title = {{{ERP CORE}}: {{An}} Open Resource for Human Event-Related Potential Research},
  shorttitle = {{{ERP CORE}}},
  author = {Kappenman, Emily S. and Farrens, Jaclyn L. and Zhang, Wendy and Stewart, Andrew X. and Luck, Steven J.},
  date = {2021-01-15},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {225},
  pages = {117465},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2020.117465},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811920309502},
  urldate = {2024-05-15},
  abstract = {Event-related potentials (ERPs) are noninvasive measures of human brain activity that index a range of sensory, cognitive, affective, and motor processes. Despite their broad application across basic and clinical research, there is little standardization of ERP paradigms and analysis protocols across studies. To address this, we created ERP CORE (Compendium of Open Resources and Experiments), a set of optimized paradigms, experiment control scripts, data processing pipelines, and sample data (N~=~40 neurotypical young adults) for seven widely used ERP components: N170, mismatch negativity (MMN), N2pc, N400, P3, lateralized readiness potential (LRP), and error-related negativity (ERN). This resource makes it possible for researchers to 1) employ standardized ERP paradigms in their research, 2) apply carefully designed analysis pipelines and use a priori selected parameters for data processing, 3) rigorously assess the quality of their data, and 4) test new analytic techniques with standardized data from a wide range of paradigms.},
  keywords = {Data quality,EEG,Event-related potentials,Open science,Reproducibility},
  file = {/Users/clarabehnke/Zotero/storage/L22234WA/Kappenman et al. - 2021 - ERP CORE An open resource for human event-related.pdf;/Users/clarabehnke/Zotero/storage/TRUTKU4W/S1053811920309502.html}
}

@article{klein24,
  title = {Distributional {{Regression}} for {{Data Analysis}}},
  author = {Klein, Nadja},
  date = {2024-04-22},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume = {11},
  pages = {321--346},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-040722-053607},
  url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-040722-053607},
  urldate = {2024-06-27},
  abstract = {The flexible modeling of an entire distribution as a function of covariates, known as distributional regression, has seen growing interest over the past decades in both the statistics and machine learning literature. This review outlines selected state-of-the-art statistical approaches to distributional regression, complemented with alternatives from machine learning. Topics covered include the similarities and differences between these approaches, extensions, properties and limitations, estimation procedures, and the availability of software. In view of the increasing complexity and availability of large-scale data, this review also discusses the scalability of traditional estimation methods, current trends, and open challenges. Illustrations are provided using data on childhood malnutrition in Nigeria and Australian electricity prices.},
  issue = {Volume 11, 2024},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/JQXCDVXX/Klein - 2024 - Distributional Regression for Data Analysis.pdf;/Users/clarabehnke/Zotero/storage/2KGQ34UL/annurev-statistics-040722-053607.html}
}

@article{kneib_etal23,
  title = {Rage {{Against}} the {{Mean}} – {{A Review}} of {{Distributional Regression Approaches}}},
  author = {Kneib, Thomas and Silbersdorff, Alexander and Säfken, Benjamin},
  date = {2023-04-01},
  journaltitle = {Econometrics and Statistics},
  shortjournal = {Econometrics and Statistics},
  volume = {26},
  pages = {99--123},
  issn = {2452-3062},
  doi = {10.1016/j.ecosta.2021.07.006},
  url = {https://www.sciencedirect.com/science/article/pii/S2452306221000824},
  urldate = {2024-08-21},
  abstract = {Distributional regression models that overcome the traditional focus on relating the conditional mean of the response to explanatory variables and instead target either the complete conditional response distribution or more general features thereof have seen increasing interest in the past decade. The current state of distributional regression will be discussed, with a particular focus on the four most prominent model classes: (i) generalized additive models for location, scale and shape, (ii) conditional transformation models and distribution regression, (iii) density regression, and (iv) quantile and expectile regression. Characteristics of the different distributional regression approaches will be provided to establish a structured overview on the similarities and differences with respect to the required assumptions on the conditional response distribution, theoretical properties, and the availability of software implementations. In addition, challenges arising in the interpretability of distributional regression models will be discussed and all four approaches will be illustrated with an application analyzing determinants of income distributions from the German Socio-Economic Panel (GSOEP).},
  keywords = {Conditional transformation models,Density regression,Distribution regression,Expectile regression,Generalized additive models for location,Quantile regression,Scale and shape},
  file = {/Users/clarabehnke/Zotero/storage/SFPYBRXH/S2452306221000824.html}
}

@article{kruschke21,
  title = {Bayesian {{Analysis Reporting Guidelines}}},
  author = {Kruschke, John K.},
  date = {2021-10},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {5},
  number = {10},
  pages = {1282--1291},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01177-7},
  url = {https://www.nature.com/articles/s41562-021-01177-7},
  urldate = {2024-08-19},
  abstract = {Previous surveys of the literature have shown that reports of statistical analyses often lack important information, causing lack of transparency and failure of reproducibility. Editors and authors agree that guidelines for reporting should be encouraged. This Review presents a set of Bayesian analysis reporting guidelines (BARG). The BARG encompass the features of previous guidelines, while including many additional details for contemporary Bayesian analyses, with explanations. An extensive example of applying the BARG is presented. The BARG should be useful to researchers, authors, reviewers, editors, educators and students. Utilization, endorsement and promotion of the BARG may improve the quality, transparency and reproducibility of Bayesian analyses.},
  langid = {english},
  keywords = {Medical research,Psychology},
  file = {/Users/clarabehnke/Zotero/storage/DRDE2PNE/Kruschke - 2021 - Bayesian Analysis Reporting Guidelines.pdf}
}

@article{kuznetsova_etal17,
  title = {{{lmerTest Package}}: {{Tests}} in {{Linear Mixed Effects Models}}},
  shorttitle = {{{lmerTest Package}}},
  author = {Kuznetsova, Alexandra and Brockhoff, Per B. and Christensen, Rune H. B.},
  date = {2017-12-06},
  journaltitle = {Journal of Statistical Software},
  volume = {82},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v082.i13},
  url = {https://doi.org/10.18637/jss.v082.i13},
  urldate = {2024-08-07},
  abstract = {One of the frequent questions by users of the mixed model function lmer of the lme4 package has been: How can I get p values for the F and t tests for objects returned by lmer? The lmerTest package extends the 'lmerMod' class of the lme4 package, by overloading the anova and summary functions by providing p values for tests for fixed effects. We have implemented the Satterthwaite's method for approximating degrees of freedom for the t and F tests. We have also implemented the construction of Type I - III ANOVA tables. Furthermore, one may also obtain the summary as well as the anova table using the Kenward-Roger approximation for denominator degrees of freedom (based on the KRmodcomp function from the pbkrtest package). Some other convenient mixed model analysis tools such as a step method, that performs backward elimination of nonsignificant effects  -  both random and fixed, calculation of population means and multiple comparison tests together with plot facilities are provided by the package as well.},
  langid = {english},
  keywords = {ANOVA,denominator degree of freedom,linear mixed effects models,lme4,R,Satterthwaite's approximation},
  file = {/Users/clarabehnke/Zotero/storage/2DLWKX4L/Kuznetsova et al. - 2017 - lmerTest Package Tests in Linear Mixed Effects Mo.pdf}
}

@article{lee_vanpaemel18,
  title = {Determining Informative Priors for Cognitive Models},
  author = {Lee, Michael D. and Vanpaemel, Wolf},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  number = {1},
  pages = {114--127},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1238-3},
  url = {https://doi.org/10.3758/s13423-017-1238-3},
  urldate = {2024-10-18},
  abstract = {The development of cognitive models involves the creative scientific formalization of assumptions, based on theory, observation, and other relevant information. In the Bayesian approach to implementing, testing, and using cognitive models, assumptions can influence both the likelihood function of the model, usually corresponding to assumptions about psychological processes, and the prior distribution over model parameters, usually corresponding to assumptions about the psychological variables that influence those processes. The specification of the prior is unique to the Bayesian context, but often raises concerns that lead to the use of vague or non-informative priors in cognitive modeling. Sometimes the concerns stem from philosophical objections, but more often practical difficulties with how priors should be determined are the stumbling block. We survey several sources of information that can help to specify priors for cognitive models, discuss some of the methods by which this information can be formalized in a prior distribution, and identify a number of benefits of including informative priors in cognitive modeling. Our discussion is based on three illustrative cognitive models, involving memory retention, categorization, and decision making.},
  langid = {english},
  keywords = {Bayesian statistics,Cognitive modeling,Informative prior distributions,Model development,Prediction},
  file = {/Users/clarabehnke/Zotero/storage/JH4D8C9A/Lee und Vanpaemel - 2018 - Determining informative priors for cognitive model.pdf}
}

@article{lee11,
  title = {How Cognitive Modeling Can Benefit from Hierarchical {{Bayesian}} Models},
  author = {Lee, Michael D.},
  date = {2011-02-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  series = {Special {{Issue}} on {{Hierarchical Bayesian Models}}},
  volume = {55},
  number = {1},
  pages = {1--7},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2010.08.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249610001148},
  urldate = {2024-09-24},
  abstract = {Hierarchical Bayesian modeling provides a flexible and interpretable way of extending simple models of cognitive processes. To introduce this special issue, we discuss four of the most important potential hierarchical Bayesian contributions. The first involves the development of more complete theories, including accounting for variation coming from sources like individual differences in cognition. The second involves the capability to account for observed behavior in terms of the combination of multiple different cognitive processes. The third involves using a few key psychological variables to explain behavior on a wide range of cognitive tasks. The fourth involves the conceptual unification and integration of disparate cognitive models. For all of these potential contributions, we outline an appropriate general hierarchical Bayesian modeling structure. We also highlight current models that already use the hierarchical Bayesian approach, as well as identifying research areas that could benefit from its adoption.},
  file = {/Users/clarabehnke/Zotero/storage/KYRBHT5Q/S0022249610001148.html}
}

@article{lewandowski_etal09,
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  date = {2009-10-01},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2009.04.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X09000876},
  urldate = {2024-10-15},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
  keywords = {Correlation matrix,Dependence vines,Onion method,Partial correlation},
  file = {/Users/clarabehnke/Zotero/storage/7C8QUBZX/S0047259X09000876.html}
}

@article{maas_hox04,
  title = {The Influence of Violations of Assumptions on Multilevel Parameter Estimates and Their Standard Errors},
  author = {Maas, Cora J. M. and Hox, Joop J.},
  date = {2004-06-15},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {46},
  number = {3},
  pages = {427--440},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2003.08.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167947303001816},
  urldate = {2024-11-08},
  abstract = {A crucial problem in the statistical analysis of hierarchically structured data is the dependence of the observations at the lower levels. Multilevel modeling programs account for this dependence and in recent years these programs have been widely accepted. One of the major assumptions of the tests of significance used in the multilevel programs is normality of the error distributions involved. Simulations were used to assess how important this assumption is for the accuracy of multilevel parameter estimates and their standard errors. Simulations varied the number of groups, the group size, and the intraclass correlation, with the second level residual errors following one of three non-normal distributions. In addition asymptotic maximum likelihood standard errors are compared to robust (Huber/White) standard errors. The results show that non-normal residuals at the second level of the model have little or no effect on the parameter estimates. For the fixed parameters, both the maximum likelihood-based standard errors and the robust standard errors are accurate. For the parameters in the random part of the model, the maximum likelihood-based standard errors at the lowest level are accurate, while the robust standard errors are often overcorrected. The standard errors of the variances of the level-two random effects are highly inaccurate, although the robust errors do perform better than the maximum likelihood errors. For good accuracy, robust standard errors need at least 100 groups. Thus, using robust standard errors as a diagnostic tool seems to be preferable to simply relying on them to solve the problem.},
  keywords = {(robust) Standard errors,Huber/White correction,Maximum likelihood,Multilevel modeling,Sandwich estimate},
  file = {/Users/clarabehnke/Zotero/storage/NU89VAAI/Maas und Hox - 2004 - The influence of violations of assumptions on mult.pdf;/Users/clarabehnke/Zotero/storage/P8LIWMHC/S0167947303001816.html}
}

@article{marwick_etal18,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375986},
  url = {https://doi.org/10.1080/00031305.2017.1375986},
  urldate = {2024-08-19},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  keywords = {Computational science,Data science,Open source software,Reproducible research},
  file = {/Users/clarabehnke/Zotero/storage/MR3MABJJ/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf}
}

@article{matuschek_etal17,
  title = {Balancing {{Type I}} Error and Power in Linear Mixed Models},
  author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
  date = {2017-06-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {94},
  pages = {305--315},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X17300013},
  urldate = {2024-09-25},
  abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. Although LMMs have many advantages over ANOVA, like ANOVAs, setting them up for data analysis also requires some care. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr, Levy, Scheepers \& Tily, 2013), presumably to keep Type I error down to the nominal α in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, higher power is achieved without inflating Type I error rate if a model selection criterion is used to select a random effect structure that is supported by the data.},
  keywords = {Hypothesis testing,Linear mixed effect model,Power},
  file = {/Users/clarabehnke/Zotero/storage/TYCYJN9D/Matuschek et al. - 2017 - Balancing Type I error and power in linear mixed m.pdf;/Users/clarabehnke/Zotero/storage/XZ6VYXBK/S0749596X17300013.html}
}

@article{mcshane_etal19,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  date = {2019-03-29},
  journaltitle = {The American Statistician},
  volume = {73},
  pages = {235--245},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1527253},
  url = {https://doi.org/10.1080/00031305.2018.1527253},
  urldate = {2024-11-01},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  issue = {sup1},
  keywords = {Null hypothesis significance testing,p-Value,Replication,Sociology of science,Statistical significance},
  file = {/Users/clarabehnke/Zotero/storage/CGB8T3BR/McShane et al. - 2019 - Abandon Statistical Significance.pdf}
}

@article{meteyard_davies20,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A. I.},
  date = {2020-06-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104092},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300061},
  urldate = {2024-08-07},
  abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods – a survey of researchers (n~=~163) and a quasi-systematic review of papers using LMMs (n~=~400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors’ intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
  keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
  file = {/Users/clarabehnke/Zotero/storage/3IXTJ8HU/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf;/Users/clarabehnke/Zotero/storage/9XYR5VGW/S0749596X20300061.html}
}

@article{mikkola_etal23,
  title = {Prior {{Knowledge Elicitation}}: {{The Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Prior {{Knowledge Elicitation}}},
  author = {Mikkola, Petrus and Martin, Osvaldo A. and Chandramouli, Suyog and Hartmann, Marcelo and Pla, Oriol Abril and Thomas, Owen and Pesonen, Henri and Corander, Jukka and Vehtari, Aki and Kaski, Samuel and Bürkner, Paul-Christian and Klami, Arto},
  date = {2023-01},
  journaltitle = {Bayesian Analysis},
  volume = {-1},
  pages = {1--33},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/23-BA1381},
  url = {https://projecteuclid.org/journals/bayesian-analysis/advance-publication/Prior-Knowledge-Elicitation-The-Past-Present-and-Future/10.1214/23-BA1381.full},
  urldate = {2024-08-09},
  abstract = {Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. In principle, prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyse the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.},
  issue = {-1},
  keywords = {Bayesian workflow,domain knowledge,informative prior,prior distribution,prior elicitation},
  file = {/Users/clarabehnke/Zotero/storage/MWAHKVXN/Mikkola et al. - 2023 - Prior Knowledge Elicitation The Past, Present, an.pdf}
}

@article{nan_etal22,
  title = {The Spatiotemporal Characteristics of {{N170s}} for Faces and Words: {{A}} Meta-Analysis Study},
  shorttitle = {The Spatiotemporal Characteristics of {{N170s}} for Faces and Words},
  author = {Nan, Weizhi and Liu, Yuqi and Zeng, Xianqing and Yang, Weibin and Liang, Junhua and Lan, Yonglong and Fu, Shimin},
  date = {2022},
  journaltitle = {PsyCh Journal},
  volume = {11},
  number = {1},
  pages = {5--17},
  issn = {2046-0260},
  doi = {10.1002/pchj.511},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pchj.511},
  urldate = {2024-10-23},
  abstract = {N170 is a negative event-related potential (ERP) component in response to visual stimuli, such as faces. It remains controversial whether N170 reflects the specific processing of faces or can also be elicited by objects of expertise (e.g., words). In this research, we conducted a meta-analysis for the spatiotemporal characteristics of N170 of face and word stimuli from 24 studies in which both stimuli were presented for each subject. We observed that (1) both face and word stimuli can elicit conspicuous N170s and that there was no difference between the amplitude of face-N170 and word-N170; (2) there is no difference in the latencies between the two N170s; and (3) both N170s are distributed in the occipitotemporal regions but with a reversed hemispheric distribution pattern—face-N170 is more negative in the right than left occipitotemporal regions, while word-N170 is the opposite. These results showed that the face- and word-N170s are qualitatively the same but have different hemispheric lateralization advantages—N170 might be a general neural index of the expertise-dependent object-recognition process in occipitotemporal regions.},
  langid = {english},
  keywords = {face,meta-analysis,N170,spatiotemporal characteristics,word},
  file = {/Users/clarabehnke/Zotero/storage/M35B7W6U/Nan et al. - 2022 - The spatiotemporal characteristics of N170s for fa.pdf;/Users/clarabehnke/Zotero/storage/3APCJN7U/pchj.html}
}

@article{nelder_wedderburn72,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {2344614},
  eprinttype = {jstor},
  pages = {370--384},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/2344614},
  urldate = {2024-09-23},
  abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
  file = {/Users/clarabehnke/Zotero/storage/MYLWHNZE/Nelder und Wedderburn - 1972 - Generalized Linear Models.pdf}
}

@book{nicenboim_etal,
  title = {An {{Introduction}} to {{Bayesian Data Analysis}} for {{Cognitive Science}}},
  author = {Nicenboim, Bruno and Schad, Daniel and Vasishth, Shravan},
  url = {https://vasishth.github.io/bayescogsci/book/},
  urldate = {2024-06-17},
  abstract = {An introduction to Bayesian data analysis for Cognitive Science.},
  file = {/Users/clarabehnke/Zotero/storage/9HVKTAWX/book.html}
}

@article{nickerson00,
  title = {Null Hypothesis Significance Testing: {{A}} Review of an Old and Continuing Controversy},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, Raymond S.},
  date = {2000},
  journaltitle = {Psychological Methods},
  volume = {5},
  number = {2},
  pages = {241--301},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1463},
  doi = {10.1037/1082-989X.5.2.241},
  abstract = {Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing,Statistical Analysis},
  file = {/Users/clarabehnke/Zotero/storage/RQYJ9STA/2000-07827-007.html}
}

@article{nieuwland_etal18,
  title = {Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension},
  author = {Nieuwland, Mante S and Politzer-Ahles, Stephen and Heyselaar, Evelien and Segaert, Katrien and Darley, Emily and Kazanina, Nina and Von Grebmer Zu Wolfsthurn, Sarah and Bartolozzi, Federica and Kogan, Vita and Ito, Aine and Mézière, Diane and Barr, Dale J and Rousselet, Guillaume A and Ferguson, Heather J and Busch-Moreno, Simon and Fu, Xiao and Tuomainen, Jyrki and Kulakova, Eugenia and Husband, E Matthew and Donaldson, David I and Kohút, Zdenko and Rueschemeyer, Shirley-Ann and Huettig, Falk},
  editor = {Shinn-Cunningham, Barbara G},
  date = {2018-04-03},
  journaltitle = {eLife},
  volume = {7},
  pages = {e33468},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.33468},
  url = {https://doi.org/10.7554/eLife.33468},
  urldate = {2024-10-23},
  abstract = {Do people routinely pre-activate the meaning and even the phonological form of upcoming words? The most acclaimed evidence for phonological prediction comes from a 2005 Nature Neuroscience publication by DeLong, Urbach and Kutas, who observed a graded modulation of electrical brain potentials (N400) to nouns and preceding articles by the probability that people use a word to continue the sentence fragment (‘cloze’). In our direct replication study spanning 9 laboratories (N=334), pre-registered replication-analyses and exploratory Bayes factor analyses successfully replicated the noun-results but, crucially, not the article-results. Pre-registered single-trial analyses also yielded a statistically significant effect for the nouns but not the articles. Exploratory Bayesian single-trial analyses showed that the article-effect may be non-zero but is likely far smaller than originally reported and too small to observe without very large sample sizes. Our results do not support the view that readers routinely pre-activate the phonological form of predictable words.},
  keywords = {language comprehension,N400,prediction},
  file = {/Users/clarabehnke/Zotero/storage/9S8VW8DU/Nieuwland et al. - 2018 - Large-scale replication study reveals a limit on p.pdf}
}

@article{peikert_brandmaier19,
  title = {A {{Reproducible Data Analysis Workflow}} with {{R Markdown}}, {{Git}}, {{Make}}, and {{Docker}}},
  author = {Peikert, Aaron and Brandmaier, Andreas Markus},
  date = {2019-11-11},
  publisher = {OSF},
  doi = {10.31234/osf.io/8xzqy},
  url = {https://osf.io/8xzqy},
  urldate = {2024-03-07},
  abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles, and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), 2) the analysis exactly reproduces at a later point in time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible. While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Combining containerization, dependence management, version management, and dynamic document generation, the proposed workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
  langid = {american},
  file = {/Users/clarabehnke/Zotero/storage/SB6A6INU/Peikert und Brandmaier - 2019 - A Reproducible Data Analysis Workflow with R Markd.pdf}
}

@article{picton_etal00,
  title = {Guidelines for Using Human Event-Related Potentials to Study Cognition: {{Recording}} Standards and Publication Criteria},
  shorttitle = {Guidelines for Using Human Event-Related Potentials to Study Cognition},
  author = {Picton, T.w. and Bentin, S. and Berg, P. and Donchin, E. and Hillyard, S.a. and Johnson JR., R. and Miller, G.a. and Ritter, W. and Ruchkin, D.s. and Rugg, M.d. and Taylor, M.j.},
  date = {2000},
  journaltitle = {Psychophysiology},
  volume = {37},
  number = {2},
  pages = {127--152},
  issn = {1469-8986},
  doi = {10.1111/1469-8986.3720127},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1469-8986.3720127},
  urldate = {2024-08-13},
  abstract = {Event-related potentials (ERPs) recorded from the human scalp can provide important information about how the human brain normally processes information and about how this processing may go awry in neurological or psychiatric disorders. Scientists using or studying ERPs must strive to overcome the many technical problems that can occur in the recording and analysis of these potentials. The methods and the results of these ERP studies must be published in a way that allows other scientists to understand exactly what was done so that they can, if necessary, replicate the experiments. The data must then be analyzed and presented in a way that allows different studies to be compared readily. This paper presents guidelines for recording ERPs and criteria for publishing the results.},
  langid = {english},
  keywords = {Artifacts,Event-related potentials,Measurement,Methods,Statistics},
  file = {/Users/clarabehnke/Zotero/storage/9UTYWXL3/Picton et al. - 2000 - Guidelines for using human event-related potential.pdf;/Users/clarabehnke/Zotero/storage/7TFFICLV/1469-8986.html}
}

@article{roberts_pashler00,
  title = {How Persuasive Is a Good Fit? {{A}} Comment on Theory Testing},
  shorttitle = {How Persuasive Is a Good Fit?},
  author = {Roberts, Seth and Pashler, Harold},
  date = {2000},
  journaltitle = {Psychological Review},
  volume = {107},
  number = {2},
  pages = {358--367},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.107.2.358},
  abstract = {Quantitative theories with free parameters often gain credence when they closely fit data. This is a mistake. A good fit reveals nothing about the flexibility of the theory (how much it cannot fit), the variability of the data (how firmly the data rule out what the theory cannot fit), or the likelihood of other outcomes (perhaps the theory could have fit any plausible result), and a reader needs all 3 pieces of information to decide how much the fit should increase belief in the theory. The use of good fits as evidence is not supported by philosophers of science nor by the history of psychology; there seem to be no examples of a theory supported mainly by good fits that has led to demonstrable progress. A better way to test a theory with free parameters is to determine how the theory constrains possible outcomes (i.e., what it predicts), assess how firmly actual outcomes agree with those constraints, and determine if plausible alternative outcomes would have been inconsistent with the theory, allowing for the variability of the data. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Goodness of Fit,Statistical Analysis,Theory Verification},
  file = {/Users/clarabehnke/Zotero/storage/4TDCABNM/Roberts und Pashler - 2000 - How persuasive is a good fit A comment on theory .pdf;/Users/clarabehnke/Zotero/storage/BE426DZE/2000-15248-005.html}
}

@article{roy20,
  title = {Convergence {{Diagnostics}} for {{Markov Chain Monte Carlo}}},
  author = {Roy, Vivekananda},
  date = {2020-03-07},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume = {7},
  pages = {387--412},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031219-041300},
  url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041300},
  urldate = {2024-08-15},
  abstract = {Markov chain Monte Carlo (MCMC) is one of the most useful approaches to scientific computing because of its flexible construction, ease of use, and generality. Indeed, MCMC is indispensable for performing Bayesian analysis. Two critical questions that MCMC practitioners need to address are where to start and when to stop the simulation. Although a great amount of research has gone into establishing convergence criteria and stopping rules with sound theoretical foundation, in practice, MCMC users often decide convergence by applying empirical diagnostic tools. This review article discusses the most widely used MCMC convergence diagnostic tools. Some recently proposed stopping rules with firm theoretical footing are also presented. The convergence diagnostics and stopping rules are illustrated using three detailed examples.},
  issue = {Volume 7, 2020},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/22ASUW7M/Roy - 2020 - Convergence Diagnostics for Markov Chain Monte Car.pdf;/Users/clarabehnke/Zotero/storage/7674HTQN/annurev-statistics-031219-041300.html}
}

@article{shiffrin_etal08,
  title = {A {{Survey}} of {{Model Evaluation Approaches With}} a {{Tutorial}} on {{Hierarchical Bayesian Methods}}},
  author = {Shiffrin, Richard M. and Lee, Michael D. and Kim, Woojae and Wagenmakers, Eric-Jan},
  date = {2008},
  journaltitle = {Cognitive Science},
  volume = {32},
  number = {8},
  pages = {1248--1284},
  issn = {1551-6709},
  doi = {10.1080/03640210802414826},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/03640210802414826},
  urldate = {2024-09-25},
  abstract = {This article reviews current methods for evaluating models in the cognitive sciences, including theoretically based approaches, such as Bayes factors and minimum description length measures; simulation approaches, including model mimicry evaluations; and practical approaches, such as validation and generalization measures. This article argues that, although often useful in specific settings, most of these approaches are limited in their ability to give a general assessment of models. This article argues that hierarchical methods, generally, and hierarchical Bayesian methods, specifically, can provide a more thorough evaluation of models in the cognitive sciences. This article presents two worked examples of hierarchical Bayesian analyses to demonstrate how the approach addresses key questions of descriptive adequacy, parameter interference, prediction, and generalization in principled and coherent ways.},
  langid = {english},
  keywords = {Bayesian model selection,Hierarchical Bayesian modeling,Minimum description length,Model evaluation,Model mimicry,Model selection,Prequential analysis},
  file = {/Users/clarabehnke/Zotero/storage/WCY92FNQ/Shiffrin et al. - 2008 - A Survey of Model Evaluation Approaches With a Tut.pdf;/Users/clarabehnke/Zotero/storage/T2CBXWP6/03640210802414826.html}
}

@article{simmons_etal11,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  date = {2011-11-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  url = {https://doi.org/10.1177/0956797611417632},
  urldate = {2024-08-19},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/YMPD3BR4/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@online{sivula_etal22,
  title = {Uncertainty in {{Bayesian Leave-One-Out Cross-Validation Based Model Comparison}}},
  author = {Sivula, Tuomas and Magnusson, Måns and Matamoros, Asael Alonzo and Vehtari, Aki},
  date = {2022-03-17},
  eprint = {2008.10296},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2008.10296},
  urldate = {2024-11-08},
  abstract = {Leave-one-out cross-validation (LOO-CV) is a popular method for comparing Bayesian models based on their estimated predictive performance on new, unseen, data. As leave-one-out cross-validation is based on finite observed data, there is uncertainty about the expected predictive performance on new data. By modeling this uncertainty when comparing two models, we can compute the probability that one model has a better predictive performance than the other. Modeling this uncertainty well is not trivial, and for example, it is known that the commonly used standard error estimate is often too small. We study the properties of the Bayesian LOO-CV estimator and the related uncertainty estimates when comparing two models. We provide new results of the properties both theoretically in the linear regression case and empirically for multiple different models and discuss the challenges of modeling the uncertainty. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection in the skewness of the individual leave-one-out terms and the distribution of the error of the Bayesian LOO-CV estimator. We show that it is possible that the problematic skewness of the error distribution, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide practical recommendations for the users of Bayesian LOO-CV for model comparison.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Statistics - Methodology},
  file = {/Users/clarabehnke/Zotero/storage/CS4WZUB6/Sivula et al. - 2022 - Uncertainty in Bayesian Leave-One-Out Cross-Valida.pdf}
}

@article{sorensen_etal16,
  title = {Bayesian Linear Mixed Models Using {{Stan}}: {{A}} Tutorial for Psychologists, Linguists, and Cognitive Scientists},
  shorttitle = {Bayesian Linear Mixed Models Using {{Stan}}},
  author = {Sorensen, Tanner and Hohenstein, Sven and Vasishth, Shravan},
  date = {2016-10-01},
  journaltitle = {The Quantitative Methods for Psychology},
  shortjournal = {TQMP},
  volume = {12},
  number = {3},
  pages = {175--200},
  issn = {2292-1354},
  doi = {10.20982/tqmp.12.3.p175},
  url = {http://www.tqmp.org/RegularArticles/vol12-3/p175},
  urldate = {2023-12-17},
  file = {/Users/clarabehnke/Zotero/storage/VB7D4ZJM/Sorensen et al. - 2016 - Bayesian linear mixed models using Stan A tutoria.pdf}
}

@software{standevelopmentteam24,
  title = {Stan {{Modeling Language Users Guide}} and {{Reference Manual}}},
  author = {Stan Development Team, },
  date = {2024},
  url = {https://mc-stan.org},
  version = {2.35}
}

@article{szucs_ioannidis17,
  title = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}: {{A Reassessment}}},
  shorttitle = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  date = {2017-08-03},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {11},
  publisher = {Frontiers},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2017.00390/full},
  urldate = {2024-11-01},
  abstract = {{$<$}p{$>$}Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology, and biomedical science in general. We review these shortcomings and suggest that, after sustained negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out.{$<$}/p{$>$}},
  langid = {english},
  keywords = {bayesian methods,False positive findings,Null hypothesis significance testing,replication crisis,Research Methodology},
  file = {/Users/clarabehnke/Zotero/storage/C2A4YLVE/Szucs und Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{tendeiro_etal24,
  title = {Diagnosing the {{Misuse}} of the {{Bayes Factor}} in {{Applied Research}}},
  author = {Tendeiro, Jorge N. and Kiers, Henk A. L. and Hoekstra, Rink and Wong, Tsz Keung and Morey, Richard D.},
  date = {2024-01-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {1},
  pages = {25152459231213371},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231213371},
  url = {https://doi.org/10.1177/25152459231213371},
  urldate = {2024-06-25},
  abstract = {Hypothesis testing is often used for inference in the social sciences. In particular, null hypothesis significance testing (NHST) and its p value have been ubiquitous in published research for decades. Much more recently, null hypothesis Bayesian testing (NHBT) and its Bayes factor have also started to become more commonplace in applied research. Following preliminary work by Wong and colleagues, we investigated how, and to what extent, researchers misapply the Bayes factor in applied psychological research by means of a literature study. Based on a final sample of 167 articles, our results indicate that, not unlike NHST and the  p  value, the use of NHBT and the Bayes factor also shows signs of misconceptions. We consider the root causes of the identified problems and provide suggestions to improve the current state of affairs. This article is aimed to assist researchers in drawing the best inferences possible while using NHBT and the Bayes factor in applied research.},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/KDN85B4S/Tendeiro et al. - 2024 - Diagnosing the Misuse of the Bayes Factor in Appli.pdf}
}

@article{tendeiro_kiers19,
  title = {A Review of Issues about Null Hypothesis {{Bayesian}} Testing},
  author = {Tendeiro, Jorge N. and Kiers, Henk A. L.},
  date = {2019},
  journaltitle = {Psychological Methods},
  volume = {24},
  number = {6},
  pages = {774--795},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000221},
  abstract = {Null hypothesis significance testing (NHST) has been under scrutiny for decades. The literature shows overwhelming evidence of a large range of problems affecting NHST. One of the proposed alternatives to NHST is using Bayes factors instead of p values. Here we denote the method of using Bayes factors to test point null models as “null hypothesis Bayesian testing” (NHBT). In this article we offer a wide overview of potential issues (limitations or sources of misinterpretation) with NHBT which is currently missing in the literature. We illustrate many of the shortcomings of NHBT by means of reproducible examples. The article concludes with a discussion of NHBT in particular and testing in general. In particular, we argue that posterior model probabilities should be given more emphasis than Bayes factors, because only the former provide direct answers to the most common research questions under consideration. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Hypothesis Testing,Models,Null Hypothesis Testing,Statistical Probability,Statistical Significance,Statistics},
  file = {/Users/clarabehnke/Zotero/storage/TKF5AJXM/Tendeiro und Kiers - 2019 - A review of issues about null hypothesis Bayesian .pdf;/Users/clarabehnke/Zotero/storage/9IP3LBKQ/2019-26880-001.html}
}

@article{tibon_levy15,
  title = {Striking a Balance: Analyzing Unbalanced Event-Related Potential Data},
  shorttitle = {Striking a Balance},
  author = {Tibon, Roni and Levy, Daniel A.},
  date = {2015-05-01},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00555},
  url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2015.00555/full},
  urldate = {2024-09-23},
  langid = {english},
  keywords = {EEG/ERP,Event-related potentials,mixed-effects models,Repeated-measures ANOVA,Unbalanced data},
  file = {/Users/clarabehnke/Zotero/storage/KWY6HFEW/Tibon und Levy - 2015 - Striking a balance analyzing unbalanced event-rel.pdf}
}

@article{vandekerckhove_etal18,
  title = {Editorial: {{Bayesian}} Methods for Advancing Psychological Science},
  shorttitle = {Editorial},
  author = {Vandekerckhove, Joachim and Rouder, Jeffrey N. and Kruschke, John K.},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  number = {1},
  pages = {1--4},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1443-8},
  url = {https://doi.org/10.3758/s13423-018-1443-8},
  urldate = {2024-11-01},
  langid = {english},
  keywords = {Bayesian inference and parameter estimation,Bayesian statistics,Evidence,New statistics},
  file = {/Users/clarabehnke/Zotero/storage/WLIBIH7V/Vandekerckhove et al. - 2018 - Editorial Bayesian methods for advancing psycholo.pdf}
}

@article{vandeschoot_etal21,
  title = {Bayesian Statistics and Modelling},
  author = {family=Schoot, given=Rens, prefix=van de, useprefix=true and Depaoli, Sarah and King, Ruth and Kramer, Bianca and Märtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher},
  date = {2021-01-14},
  journaltitle = {Nature Reviews Methods Primers},
  shortjournal = {Nat Rev Methods Primers},
  volume = {1},
  number = {1},
  pages = {1--26},
  publisher = {Nature Publishing Group},
  issn = {2662-8449},
  doi = {10.1038/s43586-020-00001-2},
  url = {https://www.nature.com/articles/s43586-020-00001-2},
  urldate = {2024-07-15},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
  langid = {english},
  keywords = {Scientific community,Statistics},
  file = {/Users/clarabehnke/Zotero/storage/KZAC22BM/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf}
}

@article{vandoorn_etal21,
  title = {The {{JASP}} Guidelines for Conducting and Reporting a {{Bayesian}} Analysis},
  author = {family=Doorn, given=Johnny, prefix=van, useprefix=true and family=Bergh, given=Don, prefix=van den, useprefix=true and Böhm, Udo and Dablander, Fabian and Derks, Koen and Draws, Tim and Etz, Alexander and Evans, Nathan J. and Gronau, Quentin F. and Haaf, Julia M. and Hinne, Max and Kucharský, Šimon and Ly, Alexander and Marsman, Maarten and Matzke, Dora and Gupta, Akash R. Komarlu Narendra and Sarafoglou, Alexandra and Stefan, Angelika and Voelkel, Jan G. and Wagenmakers, Eric-Jan},
  date = {2021-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {28},
  number = {3},
  pages = {813--826},
  issn = {1531-5320},
  doi = {10.3758/s13423-020-01798-5},
  url = {https://doi.org/10.3758/s13423-020-01798-5},
  urldate = {2024-10-31},
  abstract = {Despite the increasing popularity of Bayesian inference in empirical research, few practical guidelines provide detailed recommendations for how to apply Bayesian procedures and interpret the results. Here we offer specific guidelines for four different stages of Bayesian statistical reasoning in a research setting: planning the analysis, executing the analysis, interpreting the results, and reporting the results. The guidelines for each stage are illustrated with a running example. Although the guidelines are geared towards analyses performed with the open-source statistical software JASP, most guidelines extend to Bayesian inference in general.},
  langid = {english},
  keywords = {Bayesian inference,Scientific reporting,Statistical software},
  file = {/Users/clarabehnke/Zotero/storage/X4YQ2ULZ/van Doorn et al. - 2021 - The JASP guidelines for conducting and reporting a.pdf}
}

@article{vehtari_etal17,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  date = {2017-09-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {27},
  number = {5},
  pages = {1413--1432},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  url = {https://doi.org/10.1007/s11222-016-9696-4},
  urldate = {2024-09-03},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  langid = {english},
  keywords = {Artificial Intelligence,Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
  file = {/Users/clarabehnke/Zotero/storage/MQ2K9RA4/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf}
}

@article{vehtari_etal21,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved Rˆ}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  date = {2021-06},
  journaltitle = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full},
  urldate = {2024-09-26},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic Rˆ of Gelman and Rubin (1992) has serious flaws. Traditional Rˆ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/Users/clarabehnke/Zotero/storage/XCFAXDZQ/Vehtari et al. - 2021 - Rank-Normalization, Folding, and Localization An .pdf}
}

@article{vehtari_etal24,
  title = {Pareto {{Smoothed Importance Sampling}}},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  date = {2024},
  journaltitle = {Journal of Machine Learning Research},
  volume = {25},
  number = {72},
  pages = {1--58},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v25/19-556.html},
  urldate = {2024-09-03},
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto  k ̂~ k\textasciicircum{}  finite sample convergence rate diagnostic is useful for any Monte Carlo estimator.},
  file = {/Users/clarabehnke/Zotero/storage/TKR6AXMU/Vehtari et al. - 2024 - Pareto Smoothed Importance Sampling.pdf;/Users/clarabehnke/Zotero/storage/WEYJ9YA5/psis.html}
}

@software{vehtari_etal24a,
  title = {Loo: {{Efficient Leave-One-Out Cross-Validation}} and {{WAIC}} for {{Bayesian Models}}},
  shorttitle = {Loo},
  author = {Vehtari, Aki and Gabry, Jonah and Magnusson, Måns and Yao, Yuling and Bürkner, Paul-Christian and Paananen, Topi and Gelman, Andrew and Goodrich, Ben and Piironen, Juho and Nicenboim, Bruno and Lindgren, Leevi},
  date = {2024-07-03},
  url = {https://cran.r-project.org/web/packages/loo/index.html},
  urldate = {2024-09-03},
  abstract = {Efficient approximate leave-one-out cross-validation (LOO) for Bayesian models fit using Markov chain Monte Carlo, as described in Vehtari, Gelman, and Gabry (2017) {$<$}doi:10.1007/s11222-016-9696-4{$>$}. The approximation uses Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. As a byproduct of the calculations, we also obtain approximate standard errors for estimated predictive errors and for the comparison of predictive errors between models. The package also provides methods for using stacking and other model weighting techniques to average Bayesian predictive distributions.},
  version = {2.8.0},
  keywords = {Bayesian}
}

@article{vehtari_ojanen12,
  title = {A Survey of {{Bayesian}} Predictive Methods for Model Assessment, Selection and Comparison},
  author = {Vehtari, Aki and Ojanen, Janne},
  date = {2012-01},
  journaltitle = {Statistics Surveys},
  volume = {6},
  pages = {142--228},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/12-SS102},
  url = {https://projecteuclid.org/journals/statistics-surveys/volume-6/issue-none/A-survey-of-Bayesian-predictive-methods-for-model-assessment-selection/10.1214/12-SS102.full},
  urldate = {2024-06-06},
  abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
  issue = {none},
  keywords = {62-02,62C10,Bayesian,cross-validation,decision theory,Expected utility,information criteria,model assessment,Model selection,predictive},
  file = {/Users/clarabehnke/Zotero/storage/XKYDVLDD/Vehtari und Ojanen - 2012 - A survey of Bayesian predictive methods for model .pdf}
}

@article{volpert-esmond_etal21,
  title = {Using Multilevel Models for the Analysis of Event-Related Potentials},
  author = {Volpert-Esmond, Hannah I. and Page-Gould, Elizabeth and Bartholow, Bruce D.},
  date = {2021-04},
  journaltitle = {International Journal of Psychophysiology},
  shortjournal = {International Journal of Psychophysiology},
  volume = {162},
  pages = {145--156},
  issn = {01678760},
  doi = {10.1016/j.ijpsycho.2021.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167876021000453},
  urldate = {2024-04-17},
  langid = {english},
  file = {/Users/clarabehnke/Zotero/storage/56MULDXE/Volpert-Esmond et al. - 2021 - Using multilevel models for the analysis of event-.pdf}
}

@article{wagenmakers_etal10,
  title = {Bayesian Hypothesis Testing for Psychologists: {{A}} Tutorial on the {{Savage}}–{{Dickey}} Method},
  shorttitle = {Bayesian Hypothesis Testing for Psychologists},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
  date = {2010-05-01},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {60},
  number = {3},
  pages = {158--189},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2009.12.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028509000826},
  urldate = {2024-10-30},
  abstract = {In the field of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage–Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method’s validity, generality, and flexibility.},
  keywords = {Bayes factor,Hierarchical modeling,Model selection,Order-restrictions,Random effects,Statistical evidence},
  file = {/Users/clarabehnke/Zotero/storage/7NTUUKEZ/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf;/Users/clarabehnke/Zotero/storage/QDPHW3T6/S0010028509000826.html}
}

@article{wagenmakers_etal18,
  title = {Bayesian Inference for Psychology. {{Part I}}: {{Theoretical}} Advantages and Practical Ramifications},
  shorttitle = {Bayesian Inference for Psychology. {{Part I}}},
  author = {Wagenmakers, Eric-Jan and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Love, Jonathon and Selker, Ravi and Gronau, Quentin F. and Šmíra, Martin and Epskamp, Sacha and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  number = {1},
  pages = {35--57},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1343-3},
  url = {https://doi.org/10.3758/s13423-017-1343-3},
  urldate = {2024-11-01},
  abstract = {Bayesian parameter estimation and Bayesian hypothesis testing present attractive alternatives to classical inference using confidence intervals and p values. In part I of this series we outline ten prominent advantages of the Bayesian approach. Many of these advantages translate to concrete opportunities for pragmatic researchers. For instance, Bayesian hypothesis testing allows researchers to quantify evidence and monitor its progression as data come in, without needing to know the intention with which the data were collected. We end by countering several objections to Bayesian hypothesis testing. Part II of this series discusses JASP, a free and open source software program that makes it easy to conduct Bayesian estimation and testing for a range of popular statistical scenarios (Wagenmakers et al. this issue).},
  langid = {english},
  keywords = {Bayes factor,Hypothesis test,Posterior distribution,Statistical evidence},
  file = {/Users/clarabehnke/Zotero/storage/IFD9XDE6/Wagenmakers et al. - 2018 - Bayesian inference for psychology. Part I Theoret.pdf}
}

@online{wagenmakers_etal19,
  title = {The {{Principle}} of {{Predictive Irrelevance}}, or {{Why Intervals Should Not}} Be {{Used}} for {{Model Comparison Featuring}} a {{Point Null Hypothesis}}},
  author = {Wagenmakers, Eric-Jan and Lee, Michael and Rouder, Jeffrey N. and Morey, Richard D.},
  date = {2019-06-06},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/rqnu5},
  url = {https://osf.io/rqnu5},
  urldate = {2024-10-18},
  abstract = {The principle of predictive irrelevance states that when two competing models predict a data set equally well, that data set cannot be used to discriminate the models and --for that specific purpose-- the data set is evidentially irrelevant. To highlight the ramifications of the principle, we first show how a single binomial observation can be irrelevant in the sense that it carries no evidential value for discriminating the null hypothesis \$\textbackslash theta = 1/2\$ from a broad class of alternative hypotheses that allow \$\textbackslash theta\$ to be between 0 and 1. In contrast, the Bayesian credible interval suggest that a single binomial observation does provide some evidence against the null hypothesis. We then generalize this paradoxical result to infinitely long data sequences that are predictively irrelevant throughout. Examples feature a test of a binomial rate and a test of a normal mean. These maximally uninformative data (MUD) sequences yield credible intervals and confidence intervals that are certain to exclude the point of test as the sequence lengthens. The resolution of this paradox requires the insight that interval estimation methods --and, consequently, p values-- may not be used for model comparison involving a point null hypothesis.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Bayes factor,Confidence interval estimation,Credible interval estimation,Maximally uninformative data sequences,NML,Prediction},
  file = {/Users/clarabehnke/Zotero/storage/QS4VB4RU/Wagenmakers et al. - 2019 - The Principle of Predictive Irrelevance, or Why In.pdf}
}

@article{wagenmakers_etal21,
  title = {Seven Steps toward More Transparency in Statistical Practice},
  author = {Wagenmakers, Eric-Jan and Sarafoglou, Alexandra and Aarts, Sil and Albers, Casper and Algermissen, Johannes and Bahník, Štěpán and family=Dongen, given=Noah, prefix=van, useprefix=true and Hoekstra, Rink and Moreau, David and family=Ravenzwaaij, given=Don, prefix=van, useprefix=true and Sluga, Aljaž and Stanke, Franziska and Tendeiro, Jorge and Aczel, Balazs},
  date = {2021-11},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {5},
  number = {11},
  pages = {1473--1480},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01211-8},
  url = {https://www.nature.com/articles/s41562-021-01211-8},
  urldate = {2024-08-19},
  abstract = {We argue that statistical practice in the social and behavioural sciences benefits from transparency, a fair acknowledgement of uncertainty and openness to alternative interpretations. Here, to promote such a practice, we recommend seven concrete statistical procedures: (1) visualizing data; (2) quantifying inferential uncertainty; (3) assessing data preprocessing choices; (4) reporting multiple models; (5) involving multiple analysts; (6) interpreting results modestly; and (7) sharing data and code. We discuss their benefits and limitations, and provide guidelines for adoption. Each of the seven procedures finds inspiration in Merton’s ethos of science as reflected in the norms of communalism, universalism, disinterestedness and organized scepticism. We believe that these ethical considerations—as well as their statistical consequences—establish common ground among data analysts, despite continuing disagreements about the foundations of statistical inference.},
  langid = {english},
  keywords = {Ethics,Human behaviour},
  file = {/Users/clarabehnke/Zotero/storage/RRCRL449/Wagenmakers et al. - 2021 - Seven steps toward more transparency in statistica.pdf}
}

@article{wagenmakers07,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  date = {2007-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1531-5320},
  doi = {10.3758/BF03194105},
  url = {https://doi.org/10.3758/BF03194105},
  urldate = {2024-11-01},
  abstract = {In the field of psychology, the practice ofp value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of thep value procedure. In particular,p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover,p values do not quantify statistical evidence. This article reviews thesep value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to thesep value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
  langid = {english},
  keywords = {Bayesian Information Criterion,Null Hypothesis,Posterior Probability,Prior Distribution,Statistical Inference},
  file = {/Users/clarabehnke/Zotero/storage/LV3KQH8I/Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf}
}

@article{wilkinson_rogers73,
  title = {Symbolic {{Description}} of {{Factorial Models}} for {{Analysis}} of {{Variance}}},
  author = {Wilkinson, G. N. and Rogers, C. E.},
  date = {1973},
  journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {22},
  number = {3},
  eprint = {2346786},
  eprinttype = {jstor},
  pages = {392--399},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9254},
  doi = {10.2307/2346786},
  url = {https://www.jstor.org/stable/2346786},
  urldate = {2024-10-15},
  abstract = {The paper describes the symbolic notation and syntax for specifying factorial models for analysis of variance in the control language of the GENSTAT statistical program system at Rothamsted. The notation generalizes that of Nelder (1965). Algorithm AS 65 (Rogers, 1973) converts factorial model formulae in this notation to a list of model terms represented as binary integers. A further extension of the syntax is discussed for specifying models generally (including non-linear forms).},
  file = {/Users/clarabehnke/Zotero/storage/X34DULPZ/Wilkinson und Rogers - 1973 - Symbolic Description of Factorial Models for Analy.pdf}
}

@online{winter13,
  title = {Linear Models and Linear Mixed Effects Models in {{R}} with Linguistic Applications},
  author = {Winter, Bodo},
  date = {2013-08-26},
  eprint = {1308.5499},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1308.5499},
  url = {http://arxiv.org/abs/1308.5499},
  urldate = {2024-11-08},
  abstract = {This text is a conceptual introduction to mixed effects modeling with linguistic applications, using the R programming environment. The reader is introduced to linear modeling and assumptions, as well as to mixed effects/multilevel modeling, including a discussion of random intercepts, random slopes and likelihood ratio tests. The example used throughout the text focuses on the phonetic analysis of voice pitch data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/clarabehnke/Zotero/storage/297H8P76/Winter - 2013 - Linear models and linear mixed effects models in R.pdf;/Users/clarabehnke/Zotero/storage/7DIXTQ65/1308.html}
}
@Article{R-bridgesampling,
  title = {{bridgesampling}: An {R} Package for Estimating Normalizing Constants},
  author = {Quentin F. Gronau and Henrik Singmann and Eric-Jan Wagenmakers},
  journal = {Journal of Statistical Software},
  year = {2020},
  volume = {92},
  number = {10},
  pages = {1--29},
  doi = {10.18637/jss.v092.i10},
}
@Article{R-brms_a,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}
@Article{R-brms_b,
  title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
  author = {Paul-Christian Bürkner},
  journal = {The R Journal},
  year = {2018},
  volume = {10},
  number = {1},
  pages = {395--411},
  doi = {10.32614/RJ-2018-017},
  encoding = {UTF-8},
}
@Article{R-brms_c,
  title = {Bayesian Item Response Modeling in {R} with {brms} and {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2021},
  volume = {100},
  number = {5},
  pages = {1--54},
  doi = {10.18637/jss.v100.i05},
  encoding = {UTF-8},
}
@Manual{R-papaja,
  title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
  author = {Frederik Aust and Marius Barth},
  year = {2023},
  note = {R package version 0.1.2},
  url = {https://github.com/crsh/papaja},
}
@Article{R-lmerTest,
  title = {{lmerTest} Package: Tests in Linear Mixed Effects Models},
  author = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B. Christensen},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {82},
  number = {13},
  pages = {1--26},
  doi = {10.18637/jss.v082.i13},
}